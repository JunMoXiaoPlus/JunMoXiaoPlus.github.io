<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hadoop测试例子</title>
    <url>/archives/7caea17e/</url>
    <content><![CDATA[<ul>
<li><blockquote>
<p>微信公众号：<strong><a href="#jump_20">关注菜鸟解说大数据</a></strong><br>关注可了解更多的大数据相关的内容。问题或建议，请公众号留言;<br><strong><a href="#jump_20">如果你觉得我写的文章对你有帮助，欢迎关注和赞赏我</a><sup><a href="#fn_1" id="reffn_1">1</a></sup></strong></p>
</blockquote>
</li>
</ul>
<p>[TOC]</p>
<h3 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h3><ul>
<li>1.<a href="https://mp.weixin.qq.com/s/GOdvWHxx11pHdURx9G9hcA" target="_blank" rel="external nofollow noopener noreferrer">在腾讯云中安装mysql</a></li>
<li>2.<a href="https://mp.weixin.qq.com/s/4QHc02NnmL8rVnY0q4tohg" target="_blank" rel="external nofollow noopener noreferrer">shell脚本出来Mysql的增删改查</a></li>
<li>3.<a href="https://mp.weixin.qq.com/s/DB-d3o-Zr6ma_5LlgUfcjQ" target="_blank" rel="external nofollow noopener noreferrer">MYSQL中limit不适用的场景</a></li>
<li>4.<a href="https://mp.weixin.qq.com/s/NPzgIx-UogYBTuut-Fe5ZA" target="_blank" rel="external nofollow noopener noreferrer">用shell玩转MYSQL实战</a></li>
</ul>
<h4 id="1-找到jar的目录"><a href="#1-找到jar的目录" class="headerlink" title="1.找到jar的目录"></a>1.找到jar的目录</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">/opt/cloudera/parcels/CDH<span class="number">-6.1</span><span class="number">.1</span><span class="number">-1.</span>cdh6<span class="number">.1</span><span class="number">.1</span>.p0<span class="number">.875250</span>/jars</span><br></pre></td></tr></table></figure>
<h4 id="2-如何使用"><a href="#2-如何使用" class="headerlink" title="2.如何使用"></a>2.如何使用</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples<span class="number">-3.0</span><span class="number">.0</span>-cdh6<span class="number">.1</span><span class="number">.1</span>.jar  wordcount /tmp/data/wc.txt /tmp/data/<span class="keyword">out</span></span><br><span class="line">--说明：</span><br><span class="line">hadoop jar:执行jar的命令;</span><br><span class="line">hadoop-mapreduce-examples<span class="number">-3.0</span><span class="number">.0</span>-cdh6<span class="number">.1</span><span class="number">.1</span>.jar:执行的jar包的名字，这里因为我就在当前目录中有这个jar包所以我没有使用绝对路径，如果你不是在这个jar包所在的路径的话，需要使用绝对路径</span><br><span class="line">wordcount：程序主类的名字</span><br><span class="line">/tmp/data/wc.txt：文件的输入路径</span><br><span class="line">/tmp/data/<span class="keyword">out</span>：文件的输出路径</span><br></pre></td></tr></table></figure>
<h4 id="3-运行过程"><a href="#3-运行过程" class="headerlink" title="3.运行过程"></a>3.运行过程</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">WARNING: Use <span class="string">"yarn jar"</span> to launch YARN applications.</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">18</span> INFO client.RMProxy: Connecting to ResourceManager at hadoop01/<span class="number">192.168</span><span class="number">.163</span><span class="number">.100</span>:<span class="number">8032</span></span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">20</span> INFO mapreduce.JobResourceUploader: Disabling Erasure Coding <span class="keyword">for</span> path: /user/hdfs/.staging/job_1562050681525_0001</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">21</span> INFO input.FileInputFormat: Total input files to process : <span class="number">1</span></span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">22</span> INFO mapreduce.JobSubmitter: number of splits:<span class="number">1</span></span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">22</span> INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled <span class="keyword">is</span> deprecated. Instead, use yarn.system-metrics-publisher.enabled</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">22</span> INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_1562050681525_0001</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">22</span> INFO mapreduce.JobSubmitter: Executing with tokens: []</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">23</span> INFO conf.Configuration: resource-types.xml <span class="keyword">not</span> found</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">23</span> INFO resource.ResourceUtils: Unable to find <span class="string">'resource-types.xml'</span>.</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">25</span> INFO impl.YarnClientImpl: Submitted application application_1562050681525_0001</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">26</span> INFO mapreduce.Job: The url to track the job: http:<span class="comment">//hadoop01:8088/proxy/application_1562050681525_0001/</span></span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">25</span>:<span class="number">26</span> INFO mapreduce.Job: Running job: job_1562050681525_0001</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">26</span>:<span class="number">14</span> INFO mapreduce.Job: Job job_1562050681525_0001 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">26</span>:<span class="number">14</span> INFO mapreduce.Job:  map <span class="number">0</span>% reduce <span class="number">0</span>%</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">27</span>:<span class="number">14</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">0</span>%</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">27</span>:<span class="number">23</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">25</span>%</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">27</span>:<span class="number">28</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">50</span>%</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">27</span>:<span class="number">34</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">75</span>%</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">27</span>:<span class="number">53</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">100</span>%</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">27</span>:<span class="number">54</span> INFO mapreduce.Job: Job job_1562050681525_0001 completed successfully</span><br><span class="line"><span class="number">19</span>/<span class="number">09</span>/<span class="number">26</span> <span class="number">16</span>:<span class="number">27</span>:<span class="number">54</span> INFO mapreduce.Job: Counters: <span class="number">54</span></span><br><span class="line"> File System Counters</span><br><span class="line">  FILE: Number of bytes read=<span class="number">191</span></span><br><span class="line">  FILE: Number of bytes written=<span class="number">1088483</span></span><br><span class="line">  FILE: Number of read operations=<span class="number">0</span></span><br><span class="line">  FILE: Number of large read operations=<span class="number">0</span></span><br><span class="line">  FILE: Number of write operations=<span class="number">0</span></span><br><span class="line">  HDFS: Number of bytes read=<span class="number">221</span></span><br><span class="line">  HDFS: Number of bytes written=<span class="number">72</span></span><br><span class="line">  HDFS: Number of read operations=<span class="number">23</span></span><br><span class="line">  HDFS: Number of large read operations=<span class="number">0</span></span><br><span class="line">  HDFS: Number of write operations=<span class="number">8</span></span><br><span class="line">  HDFS: Number of bytes read erasure-coded=<span class="number">0</span></span><br><span class="line"> Job Counters </span><br><span class="line">  Launched map tasks=<span class="number">1</span></span><br><span class="line">  Launched reduce tasks=<span class="number">4</span></span><br><span class="line">  Data-local map tasks=<span class="number">1</span></span><br><span class="line">  Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=<span class="number">53647</span></span><br><span class="line">  Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=<span class="number">42839</span></span><br><span class="line">  Total time spent by all map tasks (ms)=<span class="number">53647</span></span><br><span class="line">  Total time spent by all reduce tasks (ms)=<span class="number">42839</span></span><br><span class="line">  Total vcore-milliseconds taken by all map tasks=<span class="number">53647</span></span><br><span class="line">  Total vcore-milliseconds taken by all reduce tasks=<span class="number">42839</span></span><br><span class="line">  Total megabyte-milliseconds taken by all map tasks=<span class="number">54934528</span></span><br><span class="line">  Total megabyte-milliseconds taken by all reduce tasks=<span class="number">43867136</span></span><br><span class="line"> Map-Reduce Framework</span><br><span class="line">  Map input records=<span class="number">10</span></span><br><span class="line">  Map output records=<span class="number">20</span></span><br><span class="line">  Map output bytes=<span class="number">190</span></span><br><span class="line">  Map output materialized bytes=<span class="number">175</span></span><br><span class="line">  Input split bytes=<span class="number">101</span></span><br><span class="line">  Combine input records=<span class="number">20</span></span><br><span class="line">  Combine output records=<span class="number">10</span></span><br><span class="line">  Reduce input groups=<span class="number">10</span></span><br><span class="line">  Reduce shuffle bytes=<span class="number">175</span></span><br><span class="line">  Reduce input records=<span class="number">10</span></span><br><span class="line">  Reduce output records=<span class="number">10</span></span><br><span class="line">  Spilled Records=<span class="number">20</span></span><br><span class="line">  Shuffled Maps =<span class="number">4</span></span><br><span class="line">  Failed Shuffles=<span class="number">0</span></span><br><span class="line">  Merged Map outputs=<span class="number">4</span></span><br><span class="line">  GC time elapsed (ms)=<span class="number">711</span></span><br><span class="line">  CPU time spent (ms)=<span class="number">12920</span></span><br><span class="line">  Physical memory (bytes) snapshot=<span class="number">1241501696</span></span><br><span class="line">  Virtual memory (bytes) snapshot=<span class="number">12939522048</span></span><br><span class="line">  Total committed heap usage (bytes)=<span class="number">904396800</span></span><br><span class="line">  Peak Map Physical memory (bytes)=<span class="number">450433024</span></span><br><span class="line">  Peak Map Virtual memory (bytes)=<span class="number">2578866176</span></span><br><span class="line">  Peak Reduce Physical memory (bytes)=<span class="number">206004224</span></span><br><span class="line">  Peak Reduce Virtual memory (bytes)=<span class="number">2591559680</span></span><br><span class="line"> Shuffle Errors</span><br><span class="line">  BAD_ID=<span class="number">0</span></span><br><span class="line">  CONNECTION=<span class="number">0</span></span><br><span class="line">  IO_ERROR=<span class="number">0</span></span><br><span class="line">  WRONG_LENGTH=<span class="number">0</span></span><br><span class="line">  WRONG_MAP=<span class="number">0</span></span><br><span class="line">  WRONG_REDUCE=<span class="number">0</span></span><br><span class="line"> File Input Format Counters </span><br><span class="line">  Bytes Read=<span class="number">120</span></span><br><span class="line"> File Output Format Counters </span><br><span class="line">  Bytes Written=<span class="number">72</span></span><br><span class="line">您在 /var/spool/mail/root 中有新邮件</span><br></pre></td></tr></table></figure>
<h4 id="4-结果"><a href="#4-结果" class="headerlink" title="4.结果"></a>4.结果</h4><p><img src="http://ww1.sinaimg.cn/large/006ZE1GEly1g7d0dn3w23j311m04sq37.jpg" alt="55b156eedbe6fde87c3da2046a1b0be.png"></p>
<h4 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h4><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">package org.apache.hadoop.examples;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="keyword">Configuration</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.<span class="keyword">input</span>.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="built_in">public</span> <span class="keyword">class</span> WordCount &#123;</span><br><span class="line">    <span class="built_in">public</span> WordCount() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">public</span> static <span class="type">void</span> main(String[] args) throws <span class="keyword">Exception</span> &#123;</span><br><span class="line">        <span class="keyword">Configuration</span> conf = <span class="built_in">new</span> <span class="keyword">Configuration</span>();</span><br><span class="line">        String[] otherArgs = (<span class="built_in">new</span> GenericOptionsParser(conf, args)).getRemainingArgs();</span><br><span class="line">        <span class="keyword">if</span> (otherArgs.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">System</span>.err.println("Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;");</span><br><span class="line">            <span class="keyword">System</span>.<span class="keyword">exit</span>(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf, "word count");</span><br><span class="line">        job.setJarByClass(WordCount.<span class="keyword">class</span>);</span><br><span class="line">        job.setMapperClass(WordCount.TokenizerMapper.<span class="keyword">class</span>);</span><br><span class="line">        job.setCombinerClass(WordCount.IntSumReducer.<span class="keyword">class</span>);</span><br><span class="line">        job.setReducerClass(WordCount.IntSumReducer.<span class="keyword">class</span>);</span><br><span class="line">        job.setOutputKeyClass(<span class="type">Text</span>.<span class="keyword">class</span>);</span><br><span class="line">        job.setOutputValueClass(IntWritable.<span class="keyword">class</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; otherArgs.length - <span class="number">1</span>; ++i) &#123;</span><br><span class="line">            FileInputFormat.addInputPath(job, <span class="built_in">new</span> Path(otherArgs[i]));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="built_in">new</span> Path(otherArgs[otherArgs.length - <span class="number">1</span>]));</span><br><span class="line">        <span class="keyword">System</span>.<span class="keyword">exit</span>(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">public</span> static <span class="keyword">class</span> IntSumReducer extends Reducer&lt;<span class="type">Text</span>, IntWritable, <span class="type">Text</span>, IntWritable&gt; &#123;</span><br><span class="line">        private IntWritable result = <span class="built_in">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">        <span class="built_in">public</span> IntSumReducer() &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">public</span> <span class="type">void</span> reduce(<span class="type">Text</span> key, Iterable&lt;IntWritable&gt; <span class="keyword">values</span>, Reducer&lt;<span class="type">Text</span>, IntWritable, <span class="type">Text</span>, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            IntWritable val;</span><br><span class="line">            <span class="keyword">for</span>(Iterator var5 = <span class="keyword">values</span>.iterator(); var5.hasNext(); sum += val.<span class="keyword">get</span>()) &#123;</span><br><span class="line">                val = (IntWritable)var5.next();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            this.result.<span class="keyword">set</span>(sum);</span><br><span class="line">            context.<span class="keyword">write</span>(key, this.result);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">public</span> static <span class="keyword">class</span> TokenizerMapper extends Mapper&lt;<span class="keyword">Object</span>, <span class="type">Text</span>, <span class="type">Text</span>, IntWritable&gt; &#123;</span><br><span class="line">        private static final IntWritable one = <span class="built_in">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">        private <span class="type">Text</span> word = <span class="built_in">new</span> Text();</span><br><span class="line"></span><br><span class="line">        <span class="built_in">public</span> TokenizerMapper() &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">public</span> <span class="type">void</span> map(<span class="keyword">Object</span> key, <span class="type">Text</span> <span class="keyword">value</span>, Mapper&lt;<span class="keyword">Object</span>, <span class="type">Text</span>, <span class="type">Text</span>, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            StringTokenizer itr = <span class="built_in">new</span> StringTokenizer(<span class="keyword">value</span>.toString());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(itr.hasMoreTokens()) &#123;</span><br><span class="line">                this.word.<span class="keyword">set</span>(itr.nextToken());</span><br><span class="line">                context.<span class="keyword">write</span>(this.word, one);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a id="footnote-1">&lt;/ a&gt;<br><a id="jump_20">&lt;/ a&gt;</a></a></p>
<h3 id="关注菜鸟解说大数据"><a href="#关注菜鸟解说大数据" class="headerlink" title="关注菜鸟解说大数据"></a>关注菜鸟解说大数据</h3><p>如果你觉得到作者的文章对你有帮助，欢迎赞赏，有你的支持，公众号一定会越来越好！<br><img src="https://ws1.sinaimg.cn/large/006ZE1GEly1g3r7lh0i1sj30p00f1wfx.jpg" alt="公众号二维码"></p>
]]></content>
  </entry>
  <entry>
    <title>三体中的15个思想实验</title>
    <url>/archives/8158b7bc/</url>
    <content><![CDATA[<h3 id="什么是思想实验"><a href="#什么是思想实验" class="headerlink" title="什么是思想实验"></a>什么是思想实验</h3><p>思想实验：就是在想象里设定一个极端的情况，然后沿着真实可能的逻辑推理，把可能发生的故事写出来。就是使用想象力去进行那些在现实中无法做到的实验。</p>
<h4 id="思想实验01"><a href="#思想实验01" class="headerlink" title="思想实验01"></a>思想实验01</h4><p>在信息时代，一个普通个体，能在多大程度上破坏系统。小说里的回答是完全可以毁掉一个系统。三体人1379和地球人叶文洁发生了对话，而就是两个普通个体的对话，引起了未来人类几百年的动乱，几十亿人死亡。</p>
<h4 id="思想实验02"><a href="#思想实验02" class="headerlink" title="思想实验02"></a>思想实验02</h4><p>我们知道大蛇要打七寸，那打一个文明的“七寸”，应该打什么地方。一个文明的七寸，就是科技，再具体一点是基础科学。三体人专门派出一个叫智子的东西来干扰人类对于基础科学的研究。</p>
<h4 id="思想实验03"><a href="#思想实验03" class="headerlink" title="思想实验03"></a>思想实验03</h4><p>一旦发现系统出现叛徒，最应该做的事情是什么。答案是，要获取叛徒和对手的聊天记录，对手想抢占的要点，就是你要抢占的要点。叶文洁背叛地球，与三体人沟通。地球叛军在经过巴拿马运河的时候，被地球人用纳米级的材料切割成只有50厘米厚的片状。这样可以将叛军全部摧毁，同时保留叛军与三体人的聊天记录。</p>
<h4 id="思想实验04"><a href="#思想实验04" class="headerlink" title="思想实验04"></a>思想实验04</h4><p>如果人类社会变成一个思维完全透明的世界，是好事吗？三体人的思维，是完全透明的，想和说，是一个词儿。科学角度来讲，这个技能耗散能量大、信息筛选成本高；社会学角度讲，这会加大人们协作的难度，所以，这种“想”等于“说”的信息沟通模式，不适合人类社会。</p>
<h4 id="思想实验05"><a href="#思想实验05" class="headerlink" title="思想实验05"></a>思想实验05</h4><p>如果真有外星人入侵，我们应该把决断权交给谁。最好的办法只能把决断权交给少数人，给他极大的权力，天下一盘棋。小说里的四位“面壁者”来做决策以应对三体人。</p>
<h4 id="思想实验06"><a href="#思想实验06" class="headerlink" title="思想实验06"></a>思想实验06</h4><p>如果面对外星人的侵略，我们应该采取什么样的社会架构来应对。这个取决于外星人什么时间到达地球：马上临近，短期单一任务，选择集权政府式构架，短期内提高战斗力。如果是400年以后到达地球，长期复杂任务，应该选择自由市场，释放社会活力，实现更多创新。</p>
<h4 id="思想实验07"><a href="#思想实验07" class="headerlink" title="思想实验07"></a>思想实验07</h4><p>如果在物理水平上有差距的两个文明作战，会出现什么样的情景。三体人派出“水滴”小飞船打人类2000多艘恒星级战舰。这场战役告诉我们，世界上最恨的武器是理论物理，文明间最大的差距是科学层级差距，人类所有的荣光，一朝破灭。</p>
<h4 id="思想实验08"><a href="#思想实验08" class="headerlink" title="思想实验08"></a>思想实验08</h4><p>什么是猜疑链。从末日之战逃生的七艘战舰，马上迎来一场人性的“黑暗之战”。因为燃料和配件有限，人类战舰之间爆发了一场自相残杀的战斗。最后，仅存两艘战舰，满载燃料和补给，各自朝太阳系外飞去。猜疑链是这场战争的动因，也是宇宙生存必须面临的基本概念。</p>
<h4 id="思想实验09"><a href="#思想实验09" class="headerlink" title="思想实验09"></a>思想实验09</h4><p>宇宙里的黑暗森林法则是什么。在跟罗辑见面的时候，叶文洁告诉他宇宙社会学的两个公理和两个概念，经过思考，罗辑推导出了“黑暗森林法则”：整个宇宙就像是一个黑暗森林，每个文明都是一位带枪的猎人。一旦有一个猎人发出了动静，其他文明根据猜疑链做出的第一反应就是——先消灭它再说。三体人忌惮罗辑的原因，即在于此。罗辑账务了这条法则就掌握了制约三体人的办法。</p>
<h4 id="思想实验10"><a href="#思想实验10" class="headerlink" title="思想实验10"></a>思想实验10</h4><p>如果两个势均力敌处于不稳定的平衡状态，那“强势”的一方应该注意些什么。在罗辑建立的威慑平衡里，三体是强势的一方，人类是弱势的一方。三体人通过“让地球人感知他们的真诚”和“不停寻找打破稳定的可能”这两点，为实验提供了满分答卷。在这段平衡对峙的状态里，三体人帮助地球人建设了能够威胁到他们自己的引力波发射装置，这为后来的毁灭埋下一颗种子。</p>
<h4 id="思想实验11"><a href="#思想实验11" class="headerlink" title="思想实验11"></a>思想实验11</h4><p>   一个合格的执剑人，应该具备什么性格。“执剑人”是指人类掌握黑暗打击按钮的那个人，这个角色的工作并不难：一旦发现三体人违约，就按下按钮，双方同归于尽。在罗辑卸任执剑人、继任者接过权杖的那一刻，三体人认为新的执剑人不具备同时毁灭两个文明的魄力，发起了对地球人的进攻，果然迅速扭转了对峙的局面。</p>
<h4 id="思想实验12"><a href="#思想实验12" class="headerlink" title="思想实验12"></a>思想实验12</h4><p>一个成熟的文明，应该怎么面对已经没有威胁的敌人。三体星被毁灭后，三体人跟地球领袖罗辑和程心展开了一次对话。这是一场成熟的对话，也是一个得体的告别。三体人用无对无错、无怨无恨、有礼有节的态度，结束了跟人类这场300年的对决。故事最后，三体人告诉罗辑，人类其实可以避免黑暗森林的打击，但人类更够躲开吗？</p>
<h4 id="思想实验13"><a href="#思想实验13" class="headerlink" title="思想实验13"></a>思想实验13</h4><p>在宇宙里，什么才是宇宙安全声明。阴差阳错，程心的同学云天明在三体行星生活了很久，在随三体人流浪宇宙之前，云天明通过三个故事，向地球人透露了能够躲避黑暗打击的三个方法。其中，通过降低光的速度，执行宇宙官方认为的自杀，也就是把自己所在的星系变成“黑域”，是宇宙中最明确的一种安全声明。</p>
<h4 id="思想实验14"><a href="#思想实验14" class="headerlink" title="思想实验14"></a>思想实验14</h4><p>最后把人类害死的，最可能是什么。能够逃避黑暗打击的另一个方法，是制造一种依靠“曲率驱动”的宇宙飞船。但是这种飞船带来了一个社会学问题：制造它将耗费大量的资源，最后却只能带走一小部分人，这将人类带入一个公平与道德的困境之中。但也正是这种出于道德的考量，使人类终止了飞船的建造，这一决定，最终毁灭了太阳系里的所有人类。</p>
<h4 id="思想实验15"><a href="#思想实验15" class="headerlink" title="思想实验15"></a>思想实验15</h4><p>宇宙氛围几个阵营。在太阳系遭到降维打击彻底毁灭之后，只有程心一个人逃了出来。她来到跟云天明和自己都颇有渊源的蓝星，在那里碰到了已经为人类找到新行星的关一凡。但两个人被困到空间褶皱里，再出来的时候，宇宙已经度过了1800万年。两人开始在云天明为他们准备的小宇宙里生活，彻底了解了宇宙的格局，看清了宇宙中走向死亡和阻止死亡的两大阵营。</p>
<h4 id="三体终极大点评"><a href="#三体终极大点评" class="headerlink" title="三体终极大点评"></a>三体终极大点评</h4><p>四个人物，两点收获。《三体》系列小说，成功塑造了四个主要人物，分别是：一个成功的好人，罗辑；一个失败的好人，程心；一个成功的坏人，叶文洁；一个失败的坏人，维德，展现了复杂的人性。同时给我们带来两点收获：第一要复杂，不要简单；第二要前进，不要后退。这都让我们鼓足勇气，走向更美好的明天。</p>
]]></content>
      <categories>
        <category>阅读</category>
      </categories>
      <tags>
        <tag>科幻</tag>
        <tag>科技</tag>
      </tags>
  </entry>
</search>
